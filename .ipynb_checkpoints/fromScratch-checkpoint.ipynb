{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70975bab-719e-425b-bf0c-a8c92cf1eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id            0\n",
      "age           0\n",
      "sex           0\n",
      "dataset       0\n",
      "cp            0\n",
      "trestbps     59\n",
      "chol         30\n",
      "fbs          90\n",
      "restecg       2\n",
      "thalch       55\n",
      "exang        55\n",
      "oldpeak      62\n",
      "slope       309\n",
      "ca          611\n",
      "thal        486\n",
      "num           0\n",
      "dtype: int64\n",
      "Accuracy: 0.5543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from collections import defaultdict\n",
    "\n",
    "# Softmax function for multi-class probabilities\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Custom CatBoost-like multi-class model\n",
    "class CatBoostLikeMultiClass:\n",
    "    def __init__(self, n_estimators=1000, learning_rate=0.01, max_depth=7, cat_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []  # Stores decision trees for each class\n",
    "        self.cat_features = cat_features if cat_features else []\n",
    "        self.target_encoding = {}  # Stores target encoding for categorical features\n",
    "        self.num_classes = None  # Number of target classes\n",
    "\n",
    "    # Target encoding for categorical features\n",
    "    def _target_encode(self, X, y):\n",
    "        encoded_X = X.copy()\n",
    "        self.target_encoding = {}\n",
    "        for col in self.cat_features:\n",
    "            means = defaultdict(lambda: np.mean(y))  # Default value is the mean of y\n",
    "            unique_vals = X[col].unique()\n",
    "            for val in unique_vals:\n",
    "                means[val] = np.mean(y[X[col] == val])  # Mean target value for each category\n",
    "            encoded_X[col] = X[col].map(means)\n",
    "            self.target_encoding[col] = means\n",
    "        return encoded_X\n",
    "\n",
    "    # Apply target encoding to new data\n",
    "    def _apply_target_encoding(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        for col in self.cat_features:\n",
    "            X_encoded[col] = X[col].map(self.target_encoding.get(col, {})).fillna(np.mean(list(self.target_encoding[col].values())))\n",
    "        return X_encoded\n",
    "\n",
    "    # Fit the model\n",
    "    def fit(self, X, y):\n",
    "        # Convert target to one-hot encoding\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        y_one_hot = np.eye(self.num_classes)[y]\n",
    "\n",
    "        # Encode categorical features\n",
    "        X = self._target_encode(X, y)\n",
    "\n",
    "        # Initialize predictions (logits)\n",
    "        F = np.zeros((y.shape[0], self.num_classes))\n",
    "\n",
    "        # Gradient boosting loop\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute softmax probabilities\n",
    "            p = softmax(F)\n",
    "            residuals = y_one_hot - p  # Compute residuals\n",
    "\n",
    "            # Train a tree for each class\n",
    "            trees_for_iteration = []\n",
    "            for class_idx in range(self.num_classes):\n",
    "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "                tree.fit(X, residuals[:, class_idx])  # Train tree on class-specific residuals\n",
    "                trees_for_iteration.append(tree)\n",
    "\n",
    "            self.trees.append(trees_for_iteration)\n",
    "\n",
    "            # Update predictions\n",
    "            for class_idx, tree in enumerate(trees_for_iteration):\n",
    "                F[:, class_idx] += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    # Predict probabilities\n",
    "    def predict_proba(self, X):\n",
    "        X = self._apply_target_encoding(X)\n",
    "        F = np.zeros((X.shape[0], self.num_classes))\n",
    "\n",
    "        for trees_for_iteration in self.trees:\n",
    "            for class_idx, tree in enumerate(trees_for_iteration):\n",
    "                F[:, class_idx] += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return softmax(F)\n",
    "\n",
    "    # Predict classes\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "# Load and preprocess data\n",
    "file_path = 'D:\\Python\\CatBoost\\heart_disease_uci.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data.drop(columns=['sex', 'dataset', 'id', 'ca'], inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "features = data.drop('num', axis=1)\n",
    "target = data['num']\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = ['cp', 'restecg', 'slope', 'thal']\n",
    "\n",
    "# Fill categorical NaNs with 'Missing'\n",
    "for col in categorical_features:\n",
    "    features[col] = features[col].fillna('Missing')\n",
    "\n",
    "# Fill numeric NaNs with the median\n",
    "numeric_features = features.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_features:\n",
    "    features[col] = features[col].fillna(features[col].median())\n",
    "\n",
    "# Handle binary columns with NaNs\n",
    "features['fbs_missing'] = features['fbs'].isna().astype(int)\n",
    "features['exang_missing'] = features['exang'].isna().astype(int)\n",
    "features['fbs'] = features['fbs'].fillna('FALSE').astype(str)\n",
    "features['exang'] = features['exang'].fillna('FALSE').astype(str)\n",
    "features['fbs'] = features['fbs'].replace({'True': 'TRUE', 'False': 'FALSE', 'FALSE': 'FALSE'})\n",
    "features['exang'] = features['exang'].replace({'True': 'TRUE', 'False': 'FALSE', 'FALSE': 'FALSE'})\n",
    "features['fbs'] = features['fbs'].map({'FALSE': 0, 'TRUE': 1})\n",
    "features['exang'] = features['exang'].map({'FALSE': 0, 'TRUE': 1})\n",
    "features = features.drop(['fbs_missing', 'exang_missing'], axis=1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = CatBoostLikeMultiClass(n_estimators=1000, learning_rate=0.01, max_depth=7, cat_features=categorical_features)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07d6bbc-e4cb-41b5-8947-1990303b42b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.434782608695656 %\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        75\n",
      "           1       0.46      0.39      0.42        54\n",
      "           2       0.32      0.32      0.32        25\n",
      "           3       0.48      0.38      0.43        26\n",
      "           4       0.20      0.25      0.22         4\n",
      "\n",
      "    accuracy                           0.55       184\n",
      "   macro avg       0.43      0.43      0.43       184\n",
      "weighted avg       0.54      0.55      0.54       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[62  9  3  1  0]\n",
      " [17 21 10  4  2]\n",
      " [ 4  7  8  5  1]\n",
      " [ 4  8  3 10  1]\n",
      " [ 0  1  1  1  1]]\n",
      "ROC-AUC Score: 0.7900648047065333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred) * 100, \"%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b39b29-107d-4cb4-a8f9-90f4b1ddf064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
